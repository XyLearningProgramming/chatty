# Embedding Cache System

**Status:** Planned — not yet implemented
**Created:** 2026-02-12

## Goal

Add a background cron job that pre-computes embeddings for human messages
in the DB, and a CachedChatModel wrapper that returns cached AI responses
for semantically similar queries — bypassing the LLM while preserving
logging, metrics, and DB recording.

---

## Architecture

```
Request flow:

  Incoming query
       │
  CachedChatModel._agenerate
       │
  Try acquire semaphore (instant timeout)
       │
  ┌────┴────────────┐
  │ acquired         │ timeout
  │                  └──────► Delegate to GatedChatModel (normal path)
  │
  Call /embeddings for query
  Release semaphore
       │
  Search chat_embeddings (cosine similarity)
       │
  ┌────┴────────────┐
  │ sim >= threshold  │ no match
  │ AND age < max_age │
  │                   └──────► Delegate to GatedChatModel (normal path)
  │
  Return cached AI response (no tool_calls)
  (LangChain callbacks fire naturally → DB + metrics + logging)
```

```
Cron job flow (every scan_interval):

  Read config (hot-reloadable)
       │
  Query chat_messages for human msgs without embeddings
  (LEFT JOIN chat_embeddings, ORDER BY created_at DESC, LIMIT batch_size)
       │
  Try acquire semaphore (instant timeout)
       │
  ┌────┴────────────┐
  │ acquired         │ timeout → skip, try next tick
  │
  Call /embeddings API
  Store vector in chat_embeddings
  Release semaphore
```

**Model wrapping order:**

```
CachedChatModel → GatedChatModel → ChatOpenAI
```

When CachedChatModel returns a cached ChatResult, LangChain's
BaseChatModel.agenerate() still fires on_chat_model_start and on_llm_end
callbacks, so PGMessageCallback records messages to PostgreSQL and
Prometheus metrics are emitted — identical to a normal response.

---

## Config additions

New `embedding` section in configs/config.yaml and
src/chatty/configs/system.py (EmbeddingConfig):

```yaml
embedding:
  enabled: true
  scan_interval: 300       # 5 min, hot-reloadable via ConfigMap
  batch_size: 1            # items per cron tick, hot-reloadable
  model_name: "text-embedding-ada-002"
  dimensions: 1536
  acquire_timeout: 0.1     # near-instant gate timeout (seconds)
  similarity_threshold: 0.92
  max_age: 86400           # 24h — ignore embeddings older than this
```

---

## DB schema: chat_embeddings table

New Alembic migration. ORM model ChatEmbedding added to
src/chatty/infra/db/models.py.

```sql
CREATE TABLE chat_embeddings (
    id          BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    message_id  VARCHAR NOT NULL UNIQUE,
    conversation_id VARCHAR NOT NULL,
    embedding   JSONB NOT NULL,          -- float[] stored as JSON array
    created_at  TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX ix_chat_embeddings_created_at ON chat_embeddings(created_at);
```

Embeddings stored as JSONB float arrays. Cosine similarity computed in
Python with numpy. See "Honest feedback" section for rationale.

---

## New module: src/chatty/core/embedding/

| File            | Purpose |
|-----------------|---------|
| `__init__.py`   | Re-exports |
| `client.py`     | `EmbeddingClient` — calls OpenAI-compatible `/embeddings` via `openai.AsyncOpenAI`, gated by ModelSemaphore with configurable instant acquire timeout |
| `cron.py`       | `embedding_cron_loop()` — async loop started in lifespan; each tick reads config, queries unembedded human messages, calls embedding client, stores result |
| `repository.py` | DB helpers: `find_unembedded_messages()`, `store_embedding()`, `search_similar()` (numpy cosine similarity), `load_cached_ai_response()` (next AI message without tool_calls after matched human message) |

---

## New file: src/chatty/core/llm/cached.py

CachedChatModel extends BaseChatModel, wraps GatedChatModel.

_agenerate / _astream logic:
1. Extract last human message from messages list
2. Try acquire semaphore (instant timeout); if fails, delegate to inner
3. Call /embeddings for query text, release semaphore
4. Search chat_embeddings for closest match where sim >= threshold AND
   created_at > now - max_age
5. If hit: load cached AI response from chat_messages (next role='ai'
   row without tool_calls after the matched human message), return
   ChatResult with that content
6. If miss: delegate to self.inner._agenerate(...)
7. bind_tools forwards to inner then re-binds to self (same pattern as
   GatedChatModel)

---

## Wiring changes

| File | Change |
|------|--------|
| src/chatty/configs/system.py | Add EmbeddingConfig |
| src/chatty/configs/config.py | Add `embedding: EmbeddingConfig` to AppConfig |
| configs/config.yaml | Add `embedding:` section |
| src/chatty/infra/db/models.py | Add ChatEmbedding ORM model |
| src/chatty/infra/db/__init__.py | Re-export ChatEmbedding |
| src/chatty/core/llm/dependency.py | Add `get_cached_llm()` wrapping `get_gated_llm()` |
| src/chatty/core/llm/__init__.py | Re-export CachedChatModel, get_cached_llm |
| src/chatty/core/service/dependency.py | Depend on `get_cached_llm` instead of `get_gated_llm` |
| src/chatty/app.py | Start `embedding_cron_loop` as asyncio.Task in lifespan, cancel on shutdown |
| pyproject.toml | Add `numpy` dependency |
| alembic/versions/0002_... | New migration for chat_embeddings |

---

## Prometheus metrics

- `chatty_embedding_cache_hits_total` (Counter)
- `chatty_embedding_cache_misses_total` (Counter)
- `chatty_embedding_cron_runs_total` (Counter, labels: status=ok/error/skipped)

---

## Honest feedback — things to watch for

### pgvector is overkill

cache.max_size is 30. At 30 vectors of 1536 dims, brute-force cosine
similarity in Python takes microseconds. pgvector adds a PostgreSQL
extension dependency, a new pip package, and migration complexity — all
for a search space smaller than a list comprehension. **Decision: use
JSONB array + numpy.** Revisit if the cache grows past ~1000.

### Embedding every request on the hot path adds latency on cache miss

With max_concurrency=1, when the semaphore is free the cache-check flow
is: acquire semaphore → network round-trip to /embeddings → release →
re-acquire for LLM call. That's an extra network hop on every request.
On cache miss (the majority early on), it's pure overhead.
**Mitigation: short-circuit the cache check when no embeddings exist
yet** (cheap SELECT count or a local boolean flag).

### The cron job will rarely succeed under load

With max_concurrency=1 and instant timeout, the cron only embeds when
the model is completely idle. Under steady traffic the slot is always
occupied and the cron silently skips every tick — the cache stays cold.
**Consider using a separate lightweight semaphore (or no gate) for
embedding calls**, since they're cheap API calls that don't stress the
model server the same way chat completion does.

### Cached responses break the streaming UX

The current system streams tokens progressively via SSE. A cached
response dumps the entire answer in one ContentEvent chunk — jarring
for the frontend. **Consider simulating token-by-token yield** for
cached responses (asyncio.sleep(0) between chunks of N characters).

### Cache staleness beyond max_age

If the system prompt, persona, or tool definitions change, cached
responses are stale but still served until they age out. **Mitigation:
hash the system prompt + persona config and store alongside the
embedding; reject cache hits where the hash doesn't match.**

### "Skipping tool" edge case

The final AI answer may reference tool output ("According to your
resume..."). If tool content has changed since the cached response,
the answer is factually wrong in a way max_age alone won't catch.
Acceptable risk for a personal chatbot, but worth noting.

### Multi-pod race condition

If multiple replicas run, two pods could pick up the same unembedded
message. Use SELECT ... FOR UPDATE SKIP LOCKED or a distributed lock.
Not a blocker for single-replica dev.

### Model-level vs service-level wrapping

CachedChatModel._agenerate doing DB queries, HTTP calls, and similarity
math is unusual for a LangChain model wrapper. A CachedChatService
wrapping OneStepChatService at the service layer would be more natural.
However, the model-level approach works and keeps the service untouched,
and the callback plumbing for DB/metrics comes for free. Either approach
is viable.
